{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa38025-1a01-4353-abaf-5a69c0c47c75",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cce924-9bd4-45f2-a0df-73394bacabb1",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "\n",
    "from hw3 import training\n",
    "\n",
    "\n",
    "from cs236781.plot import plot_fit\n",
    "from cs236781.train_results import FitResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626375cb-dd6d-4b0e-9d42-8c8bb234e9e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 4: Fine-Tuning a pretrained language model\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08167a-6cac-4d03-806a-e60974e81f75",
   "metadata": {},
   "source": [
    "In this part , we will deal with the fine-tuning of BERT for sentiment analysis on the IMDB movie reivews dataset from the previous section.   \n",
    "BERT is a large language model developed by Google researchers in 2019 that offers a good balance between popularity and model size, which can be fine-tuned using a simple GPU.  \n",
    "\n",
    "If you aren't yet familiar, you can check it out here:  \n",
    "https://arxiv.org/pdf/1810.04805.pdf.\n",
    "(Read Section 3 for details on the model architecture and fine-tuning on downstream tasks).\n",
    "\n",
    "In particular, we will use the distilled (smaller) version of BERT, called Distil-BERT.\n",
    "Distil-BERT is widely used in production since it has 40% fewer parameters than BERT, while running 60% faster and retaining 95% of the performance in many benchmarks.\n",
    "It is recommended to glance through the Distil-BERT paper to get a feel for the model architecture and how it differs from BERT: \n",
    "https://arxiv.org/pdf/1910.01108.pdf\n",
    "\n",
    "We will download a pre-trained `Distil-BERT` from `Hugging Face`, so there is no need to train it from scratch. \n",
    "\n",
    "One of the key strengths of Hugging Face is its extensive collection of pre-trained models. These models are trained on large-scale datasets and exhibit impressive performance on various NLP tasks, such as text classification, named entity recognition, sentiment analysis, machine translation, and question answering, among others. The pre-trained models provided by Hugging Face can be easily fine-tuned for specific downstream tasks, saving significant time and computational resources.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61922e-8ee9-40bd-8abc-7b98d5515f0b",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fe1ca-c764-4186-bfee-7a80e8caa0a8",
   "metadata": {},
   "source": [
    "We will now load and prepare the IMDB dataset as we did in the previous part.  \n",
    "Here we will load the full training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81cd1ad-5ab0-418a-a339-4e7f7b95666f",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Found cached dataset imdb (/home/noam.moshe/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2be842e81f594dd68625037acf84f768"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('imdb', split=['train', 'test[12260:12740]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146165ca-7636-46fe-bd16-1e964d6c41b9",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[Dataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n}), Dataset({\n    features: ['text', 'label'],\n    num_rows: 480\n})]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dead5a-4b2e-400f-8551-c7fd44179627",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#wrap it in a DatasetDict to enable methods such as map and format\n",
    "dataset = DatasetDict({'train': dataset[0], 'test': dataset[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07b25d3-0912-4586-b60b-6dc66e6746bc",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 480\n    })\n})"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6ef0b-70e3-4ec5-83fb-b8b779237ef6",
   "metadata": {},
   "source": [
    "We can now access the datasets in the Dict as we would a dictionary.\n",
    "Let's print a few training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dccf69f-4f53-4f96-a882-c6ca6a5e0b3c",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "TRAINING SAMPLE 0:\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\nLabel 0: 0\n\n\nTRAINING SAMPLE 1:\n\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\nLabel 1: 0\n\n\nTRAINING SAMPLE 2:\nIf only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\nLabel 2: 0\n\n\nTRAINING SAMPLE 3:\nThis film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\nLabel 3: 0\n\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f'TRAINING SAMPLE {i}:') \n",
    "    print(dataset['train'][i]['text'])\n",
    "    label = dataset['train'][i]['label']\n",
    "    print(f'Label {i}: {label}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea96efc-beee-4bd0-9810-ae4d3a4f2da8",
   "metadata": {},
   "source": [
    "We should also check the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb8333b-9271-41b8-845f-3f1694d93145",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "negative samples in train dataset: 12500\npositive samples in train dataset: 12500\nnegative samples in test dataset: 240\npositive samples in test dataset: 240\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def label_cnt(type):\n",
    "    ds = dataset[type]\n",
    "    size = len(ds)\n",
    "    cnt= 0 \n",
    "    for smp in ds:\n",
    "        cnt += smp['label']\n",
    "    print(f'negative samples in {type} dataset: {size - cnt}')\n",
    "    print(f'positive samples in {type} dataset: {cnt}')\n",
    "    \n",
    "label_cnt('train')\n",
    "label_cnt('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb5437-c7ca-45ff-ba09-43cc8a8bb916",
   "metadata": {},
   "source": [
    "### __Import the tokenizer for the dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e913e8-9dbc-4f3a-8315-920e016082d1",
   "metadata": {},
   "source": [
    "\n",
    "We will now tokenize the text the same way we did in the previous part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1f2f64-07ad-4260-a790-55a0c6f636b5",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Tokenizer input max length: 512\nTokenizer vocabulary size: 30522\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc753304-4f5a-4ec5-ac2c-04a02f85c68e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/noam.moshe/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-2c94583cc9a60a41.arrow\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/480 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3b188efc7fd418281ec5fe83e286287"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_tokenized = dataset.map(tokenize_text, batched=True, batch_size =None)\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = tokenize_dataset(dataset)\n",
    "# we would like to work with pytorch so we can manually fine-tune\n",
    "dataset_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# no need to parrarelize in this assignment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f20fb7-d8f7-41a7-8464-33b5ab488eea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the dataloaders and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bb695-5ebc-4927-983e-9880fedf3627",
   "metadata": {},
   "source": [
    "We will now set up the dataloaders for efficient batching and loading of the data.  \n",
    "By now, you are familiar with the Class methods that are needed to create a working Dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a6770a-d5d1-4fb0-a7e7-bb15e0cba57b",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ds[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e7119d-62fb-4e80-915d-79d25edde074",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(dataset_tokenized['train'])\n",
    "test_dataset = IMDBDataset(dataset_tokenized['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b332f651-1a7c-45d4-949f-a09ad2c635db",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "n_workers= 0\n",
    "\n",
    "dl_train,dl_test = [ \n",
    "    DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=n_workers\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6401fe-0590-4dec-8679-b9d0e36123d7",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x7f38800f62b0>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "dl_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f020ce-572c-4e8f-869e-5b92f36b6313",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Importing the model from Hugging Face\n",
    "We will now  delve into the process of loading the DistilBERT model from `Hugging Face`. DistilBERT is a distilled version of the BERT model, offering a lighter and faster alternative while retaining considerable performance on various NLP tasks.  \n",
    "Please refer to the introduction to check out the relevant papers.  \n",
    "For more info on how to use this model, feel free to check it out on the site:  \n",
    "https://huggingface.co/distilbert-base-uncased \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee7eef-3ce5-4026-8731-316dc4f2c53b",
   "metadata": {},
   "source": [
    "To begin, we will import the necessary library required for our implementation.\n",
    "It is fine if you receive a warning from `Hugging Face` to train the model on a downstream task, which is exactly what we will do on our IMDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a936c1-b62f-4be3-a7e0-dc4a25af1686",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9135d51-fe93-48b3-bbd1-327d475b983c",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "315a0f047df14f1c86010f67ae9e9772"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6993c8-c9d0-4aa5-a1bf-a7ff4555cc9b",
   "metadata": {},
   "source": [
    "__Let's print the model architecture to see what we are dealing with:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30274a2c-d2ac-407f-8aa2-ba2cd531d044",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c63e3-0e26-4af9-91db-5bec4bb7153e",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "We will now move on to the process of fine-tuning the model that we previously loaded from `Hugging Face`. Fine-tuning allows us to adapt the pre-trained model to our specific NLP task by further training it on task-specific data. This process enhances the model's performance and enables it to make more accurate predictions on our target task.  \n",
    "\n",
    "There are generally two approaches to fine-tuning the loaded model, each with its own advantages and considerations:  \n",
    "\n",
    "1) __Freeze all the weights besides the last two linear layers and train only those layers__:  \n",
    "This approach is commonly referred to as \"transfer learning\" or \"feature extraction.\" By freezing the weights of the majority of the model's layers, we retain the pre-trained knowledge captured by the model, allowing it to extract useful features from our data. We then replace and train the final few layers, typically linear layers, to adapt the model to our specific task. This method is beneficial when we have limited labeled data or when the pre-trained model has been trained on a similar domain.\n",
    "\n",
    "2) __Retrain all the parameters in the model__:  \n",
    "This approach involves unfreezing and training all the parameters of the loaded model, including the pre-trained layers. By retraining all the parameters, we allow the model to adjust its representations and update its knowledge based on our specific task and data. This method is often preferred when we have sufficient labeled data available and want the model to learn task-specific features from scratch or when the pre-trained model's knowledge may not be directly applicable to our domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccd1bb-7e4f-4aa0-897f-1f8d73639543",
   "metadata": {},
   "source": [
    "### Fine-tuning method 1 \n",
    "__Freeze all the weights besides the last two linear layers and train only those layers__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876d7a82-2233-4017-8221-26a46f9816a9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Freeze all parameters except for the last 2 linear layers\n",
    "# ====== YOUR CODE: ======\n",
    "params_to_train = ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True if name in params_to_train else False\n",
    "# ========================\n",
    "\n",
    "# HINT: use the printed model architecture to get the layer names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c125d-d1e9-480c-b527-ffd8e94a19ed",
   "metadata": {},
   "source": [
    "### Training \n",
    "We can use our abstract __Trainer__ class to fine-tune the model:\n",
    "We will not play around with hyperparameters in this section, as the point is to learn to fine-tune a model.   \n",
    "In addition, we do not need to send our own loss function for this loaded model (try to understand why).   \n",
    "\n",
    "__TODO__: Implement the `FineTuningTrainer` in `hw3/training.py`\n",
    "\n",
    "We will train the model for 2 epochs of 40 batches.  \n",
    "You can run this either locally or on the course servers, whichever is most comfortable for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ed2f33-018c-4a1b-b5cc-2f9c62de89c4",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "--- EPOCH 1/2 ---\n",
      "\rtrain_batch:   0%|                                                                                                                                     | 0/40 [00:00<?, ?it/s]",
      "\rtrain_batch (0.684):   0%|                                                                                                                             | 0/40 [00:11<?, ?it/s]",
      "\rtrain_batch (0.684):   2%|██▉                                                                                                                  | 1/40 [00:11<07:27, 11.48s/it]",
      "\rtrain_batch (0.699):   2%|██▉                                                                                                                  | 1/40 [00:22<07:27, 11.48s/it]",
      "\rtrain_batch (0.699):   5%|█████▊                                                                                                               | 2/40 [00:22<07:16, 11.48s/it]",
      "\rtrain_batch (0.679):   5%|█████▊                                                                                                               | 2/40 [00:34<07:16, 11.48s/it]",
      "\rtrain_batch (0.679):   8%|████████▊                                                                                                            | 3/40 [00:34<07:04, 11.46s/it]",
      "\rtrain_batch (0.676):   8%|████████▊                                                                                                            | 3/40 [00:45<07:04, 11.46s/it]",
      "\rtrain_batch (0.676):  10%|███████████▋                                                                                                         | 4/40 [00:45<06:52, 11.46s/it]",
      "\rtrain_batch (0.655):  10%|███████████▋                                                                                                         | 4/40 [00:57<06:52, 11.46s/it]",
      "\rtrain_batch (0.655):  12%|██████████████▋                                                                                                      | 5/40 [00:57<06:40, 11.43s/it]",
      "\rtrain_batch (0.665):  12%|██████████████▋                                                                                                      | 5/40 [01:08<06:40, 11.43s/it]",
      "\rtrain_batch (0.665):  15%|█████████████████▌                                                                                                   | 6/40 [01:08<06:28, 11.42s/it]",
      "\rtrain_batch (0.674):  15%|█████████████████▌                                                                                                   | 6/40 [01:20<06:28, 11.42s/it]",
      "\rtrain_batch (0.674):  18%|████████████████████▍                                                                                                | 7/40 [01:20<06:16, 11.42s/it]",
      "\rtrain_batch (0.651):  18%|████████████████████▍                                                                                                | 7/40 [01:31<06:16, 11.42s/it]",
      "\rtrain_batch (0.651):  20%|███████████████████████▍                                                                                             | 8/40 [01:31<06:07, 11.49s/it]",
      "\rtrain_batch (0.663):  20%|███████████████████████▍                                                                                             | 8/40 [01:43<06:07, 11.49s/it]",
      "\rtrain_batch (0.663):  22%|██████████████████████████▎                                                                                          | 9/40 [01:43<05:57, 11.55s/it]",
      "\rtrain_batch (0.686):  22%|██████████████████████████▎                                                                                          | 9/40 [01:55<05:57, 11.55s/it]",
      "\rtrain_batch (0.686):  25%|█████████████████████████████                                                                                       | 10/40 [01:55<05:47, 11.59s/it]",
      "\rtrain_batch (0.672):  25%|█████████████████████████████                                                                                       | 10/40 [02:06<05:47, 11.59s/it]",
      "\rtrain_batch (0.672):  28%|███████████████████████████████▉                                                                                    | 11/40 [02:06<05:36, 11.61s/it]",
      "\rtrain_batch (0.701):  28%|███████████████████████████████▉                                                                                    | 11/40 [02:18<05:36, 11.61s/it]",
      "\rtrain_batch (0.701):  30%|██████████████████████████████████▊                                                                                 | 12/40 [02:18<05:25, 11.63s/it]",
      "\rtrain_batch (0.667):  30%|██████████████████████████████████▊                                                                                 | 12/40 [02:29<05:25, 11.63s/it]",
      "\rtrain_batch (0.667):  32%|█████████████████████████████████████▋                                                                              | 13/40 [02:29<05:13, 11.63s/it]",
      "\rtrain_batch (0.642):  32%|█████████████████████████████████████▋                                                                              | 13/40 [02:41<05:13, 11.63s/it]",
      "\rtrain_batch (0.642):  35%|████████████████████████████████████████▌                                                                           | 14/40 [02:41<05:02, 11.62s/it]",
      "\rtrain_batch (0.655):  35%|████████████████████████████████████████▌                                                                           | 14/40 [02:53<05:02, 11.62s/it]",
      "\rtrain_batch (0.655):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [02:53<04:50, 11.62s/it]",
      "\rtrain_batch (0.661):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [03:04<04:50, 11.62s/it]",
      "\rtrain_batch (0.661):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [03:04<04:39, 11.63s/it]",
      "\rtrain_batch (0.683):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [03:16<04:39, 11.63s/it]",
      "\rtrain_batch (0.683):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [03:16<04:26, 11.60s/it]",
      "\rtrain_batch (0.640):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [03:27<04:26, 11.60s/it]",
      "\rtrain_batch (0.640):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [03:27<04:13, 11.54s/it]",
      "\rtrain_batch (0.669):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [03:39<04:13, 11.54s/it]",
      "\rtrain_batch (0.669):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [03:39<04:01, 11.49s/it]",
      "\rtrain_batch (0.668):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [03:50<04:01, 11.49s/it]",
      "\rtrain_batch (0.668):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [03:50<03:49, 11.47s/it]",
      "\rtrain_batch (0.637):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [04:01<03:49, 11.47s/it]",
      "\rtrain_batch (0.637):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [04:01<03:37, 11.44s/it]",
      "\rtrain_batch (0.639):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [04:13<03:37, 11.44s/it]",
      "\rtrain_batch (0.639):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [04:13<03:25, 11.44s/it]",
      "\rtrain_batch (0.645):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [04:25<03:25, 11.44s/it]",
      "\rtrain_batch (0.645):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [04:25<03:15, 11.51s/it]",
      "\rtrain_batch (0.638):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [04:36<03:15, 11.51s/it]",
      "\rtrain_batch (0.638):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [04:36<03:04, 11.54s/it]",
      "\rtrain_batch (0.646):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [04:48<03:04, 11.54s/it]",
      "\rtrain_batch (0.646):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [04:48<02:53, 11.59s/it]",
      "\rtrain_batch (0.660):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [05:00<02:53, 11.59s/it]",
      "\rtrain_batch (0.660):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [05:00<02:42, 11.60s/it]",
      "\rtrain_batch (0.655):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [05:11<02:42, 11.60s/it]",
      "\rtrain_batch (0.655):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [05:11<02:30, 11.57s/it]",
      "\rtrain_batch (0.637):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [05:22<02:30, 11.57s/it]",
      "\rtrain_batch (0.637):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [05:22<02:18, 11.53s/it]",
      "\rtrain_batch (0.697):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [05:34<02:18, 11.53s/it]",
      "\rtrain_batch (0.697):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [05:34<02:06, 11.49s/it]",
      "\rtrain_batch (0.664):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [05:45<02:06, 11.49s/it]",
      "\rtrain_batch (0.664):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [05:45<01:54, 11.47s/it]",
      "\rtrain_batch (0.686):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [05:57<01:54, 11.47s/it]",
      "\rtrain_batch (0.686):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [05:57<01:43, 11.46s/it]",
      "\rtrain_batch (0.686):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [06:08<01:43, 11.46s/it]",
      "\rtrain_batch (0.686):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [06:08<01:31, 11.44s/it]",
      "\rtrain_batch (0.614):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [06:19<01:31, 11.44s/it]",
      "\rtrain_batch (0.614):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [06:19<01:19, 11.43s/it]",
      "\rtrain_batch (0.655):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [06:31<01:19, 11.43s/it]",
      "\rtrain_batch (0.655):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [06:31<01:08, 11.41s/it]",
      "\rtrain_batch (0.624):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [06:42<01:08, 11.41s/it]",
      "\rtrain_batch (0.624):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [06:42<00:56, 11.40s/it]",
      "\rtrain_batch (0.658):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [06:54<00:56, 11.40s/it]",
      "\rtrain_batch (0.658):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [06:54<00:45, 11.40s/it]",
      "\rtrain_batch (0.663):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [07:05<00:45, 11.40s/it]",
      "\rtrain_batch (0.663):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [07:05<00:34, 11.40s/it]",
      "\rtrain_batch (0.647):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [07:16<00:34, 11.40s/it]",
      "\rtrain_batch (0.647):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [07:16<00:22, 11.41s/it]",
      "\rtrain_batch (0.693):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [07:28<00:22, 11.41s/it]",
      "\rtrain_batch (0.693):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [07:28<00:11, 11.41s/it]",
      "\rtrain_batch (0.662):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [07:39<00:11, 11.41s/it]",
      "\rtrain_batch (0.662): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:39<00:00, 11.40s/it]",
      "\rtrain_batch (Avg. Loss 0.662, Accuracy 67.7): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:39<00:00, 11.40s/it]",
      "\rtrain_batch (Avg. Loss 0.662, Accuracy 67.7): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:39<00:00, 11.49s/it]",
      "\n",
      "\rtest_batch:   0%|                                                                                                                                      | 0/40 [00:00<?, ?it/s]",
      "\rtest_batch (0.714):   0%|                                                                                                                              | 0/40 [00:09<?, ?it/s]",
      "\rtest_batch (0.714):   2%|██▉                                                                                                                   | 1/40 [00:09<06:13,  9.58s/it]",
      "\rtest_batch (0.677):   2%|██▉                                                                                                                   | 1/40 [00:19<06:13,  9.58s/it]",
      "\rtest_batch (0.677):   5%|█████▉                                                                                                                | 2/40 [00:19<06:04,  9.60s/it]",
      "\rtest_batch (0.715):   5%|█████▉                                                                                                                | 2/40 [00:28<06:04,  9.60s/it]",
      "\rtest_batch (0.715):   8%|████████▊                                                                                                             | 3/40 [00:28<05:54,  9.59s/it]",
      "\rtest_batch (0.707):   8%|████████▊                                                                                                             | 3/40 [00:38<05:54,  9.59s/it]",
      "\rtest_batch (0.707):  10%|███████████▊                                                                                                          | 4/40 [00:38<05:45,  9.60s/it]",
      "\rtest_batch (0.699):  10%|███████████▊                                                                                                          | 4/40 [00:47<05:45,  9.60s/it]",
      "\rtest_batch (0.699):  12%|██████████████▊                                                                                                       | 5/40 [00:48<05:36,  9.60s/it]",
      "\rtest_batch (0.658):  12%|██████████████▊                                                                                                       | 5/40 [00:57<05:36,  9.60s/it]",
      "\rtest_batch (0.658):  15%|█████████████████▋                                                                                                    | 6/40 [00:57<05:26,  9.60s/it]",
      "\rtest_batch (0.689):  15%|█████████████████▋                                                                                                    | 6/40 [01:07<05:26,  9.60s/it]",
      "\rtest_batch (0.689):  18%|████████████████████▋                                                                                                 | 7/40 [01:07<05:16,  9.60s/it]",
      "\rtest_batch (0.715):  18%|████████████████████▋                                                                                                 | 7/40 [01:16<05:16,  9.60s/it]",
      "\rtest_batch (0.715):  20%|███████████████████████▌                                                                                              | 8/40 [01:16<05:06,  9.59s/it]",
      "\rtest_batch (0.732):  20%|███████████████████████▌                                                                                              | 8/40 [01:26<05:06,  9.59s/it]",
      "\rtest_batch (0.732):  22%|██████████████████████████▌                                                                                           | 9/40 [01:26<04:57,  9.59s/it]",
      "\rtest_batch (0.688):  22%|██████████████████████████▌                                                                                           | 9/40 [01:35<04:57,  9.59s/it]",
      "\rtest_batch (0.688):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:35<04:47,  9.59s/it]",
      "\rtest_batch (0.657):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:45<04:47,  9.59s/it]",
      "\rtest_batch (0.657):  28%|████████████████████████████████▏                                                                                    | 11/40 [01:45<04:38,  9.60s/it]",
      "\rtest_batch (0.643):  28%|████████████████████████████████▏                                                                                    | 11/40 [01:55<04:38,  9.60s/it]",
      "\rtest_batch (0.643):  30%|███████████████████████████████████                                                                                  | 12/40 [01:55<04:28,  9.60s/it]",
      "\rtest_batch (0.688):  30%|███████████████████████████████████                                                                                  | 12/40 [02:04<04:28,  9.60s/it]",
      "\rtest_batch (0.688):  32%|██████████████████████████████████████                                                                               | 13/40 [02:04<04:19,  9.60s/it]",
      "\rtest_batch (0.765):  32%|██████████████████████████████████████                                                                               | 13/40 [02:14<04:19,  9.60s/it]",
      "\rtest_batch (0.765):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:14<04:09,  9.61s/it]",
      "\rtest_batch (0.722):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:24<04:09,  9.61s/it]",
      "\rtest_batch (0.722):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:24<04:00,  9.64s/it]",
      "\rtest_batch (0.701):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:33<04:00,  9.64s/it]",
      "\rtest_batch (0.701):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:33<03:50,  9.62s/it]",
      "\rtest_batch (0.688):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:43<03:50,  9.62s/it]",
      "\rtest_batch (0.688):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [02:43<03:41,  9.64s/it]",
      "\rtest_batch (0.667):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [02:52<03:41,  9.64s/it]",
      "\rtest_batch (0.667):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [02:52<03:31,  9.63s/it]",
      "\rtest_batch (0.646):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [03:02<03:31,  9.63s/it]",
      "\rtest_batch (0.646):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:02<03:21,  9.62s/it]",
      "\rtest_batch (0.682):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:12<03:21,  9.62s/it]",
      "\rtest_batch (0.682):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:12<03:12,  9.62s/it]",
      "\rtest_batch (0.577):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:21<03:12,  9.62s/it]",
      "\rtest_batch (0.577):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:21<03:02,  9.61s/it]",
      "\rtest_batch (0.572):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:31<03:02,  9.61s/it]",
      "\rtest_batch (0.572):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:31<02:52,  9.60s/it]",
      "\rtest_batch (0.607):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:40<02:52,  9.60s/it]",
      "\rtest_batch (0.607):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [03:40<02:43,  9.60s/it]",
      "\rtest_batch (0.601):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [03:50<02:43,  9.60s/it]",
      "\rtest_batch (0.601):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [03:50<02:33,  9.60s/it]",
      "\rtest_batch (0.575):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [04:00<02:33,  9.60s/it]",
      "\rtest_batch (0.575):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [04:00<02:24,  9.60s/it]",
      "\rtest_batch (0.537):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [04:09<02:24,  9.60s/it]",
      "\rtest_batch (0.537):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:09<02:14,  9.60s/it]",
      "\rtest_batch (0.541):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:19<02:14,  9.60s/it]",
      "\rtest_batch (0.541):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:19<02:04,  9.60s/it]",
      "\rtest_batch (0.526):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:29<02:04,  9.60s/it]",
      "\rtest_batch (0.526):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:29<01:55,  9.61s/it]",
      "\rtest_batch (0.574):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:38<01:55,  9.61s/it]",
      "\rtest_batch (0.574):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [04:38<01:45,  9.62s/it]",
      "\rtest_batch (0.574):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [04:48<01:45,  9.62s/it]",
      "\rtest_batch (0.574):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [04:48<01:36,  9.62s/it]",
      "\rtest_batch (0.599):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [04:57<01:36,  9.62s/it]",
      "\rtest_batch (0.599):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [04:57<01:26,  9.62s/it]",
      "\rtest_batch (0.569):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [05:07<01:26,  9.62s/it]",
      "\rtest_batch (0.569):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:07<01:16,  9.61s/it]",
      "\rtest_batch (0.601):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:17<01:16,  9.61s/it]",
      "\rtest_batch (0.601):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:17<01:07,  9.62s/it]",
      "\rtest_batch (0.674):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:26<01:07,  9.62s/it]",
      "\rtest_batch (0.674):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [05:26<00:57,  9.61s/it]",
      "\rtest_batch (0.614):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [05:36<00:57,  9.61s/it]",
      "\rtest_batch (0.614):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [05:36<00:48,  9.61s/it]",
      "\rtest_batch (0.580):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [05:45<00:48,  9.61s/it]",
      "\rtest_batch (0.580):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [05:45<00:38,  9.61s/it]",
      "\rtest_batch (0.542):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [05:55<00:38,  9.61s/it]",
      "\rtest_batch (0.542):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [05:55<00:28,  9.61s/it]",
      "\rtest_batch (0.640):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [06:05<00:28,  9.61s/it]",
      "\rtest_batch (0.640):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:05<00:19,  9.61s/it]",
      "\rtest_batch (0.619):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:14<00:19,  9.61s/it]",
      "\rtest_batch (0.619):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:14<00:09,  9.62s/it]",
      "\rtest_batch (0.562):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:24<00:09,  9.62s/it]",
      "\rtest_batch (0.562): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:24<00:00,  9.61s/it]",
      "\rtest_batch (Avg. Loss 0.638, Accuracy 74.0): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:24<00:00,  9.61s/it]",
      "\rtest_batch (Avg. Loss 0.638, Accuracy 74.0): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:24<00:00,  9.61s/it]",
      "\n",
      "*** Saved checkpoint finetuned_last_2.pt at epoch 1\n--- EPOCH 2/2 ---\n",
      "\rtrain_batch:   0%|                                                                                                                                     | 0/40 [00:00<?, ?it/s]",
      "\rtrain_batch (0.662):   0%|                                                                                                                             | 0/40 [00:11<?, ?it/s]",
      "\rtrain_batch (0.662):   2%|██▉                                                                                                                  | 1/40 [00:11<07:25, 11.44s/it]",
      "\rtrain_batch (0.649):   2%|██▉                                                                                                                  | 1/40 [00:22<07:25, 11.44s/it]",
      "\rtrain_batch (0.649):   5%|█████▊                                                                                                               | 2/40 [00:22<07:16, 11.48s/it]",
      "\rtrain_batch (0.676):   5%|█████▊                                                                                                               | 2/40 [00:34<07:16, 11.48s/it]",
      "\rtrain_batch (0.676):   8%|████████▊                                                                                                            | 3/40 [00:34<07:03, 11.46s/it]",
      "\rtrain_batch (0.689):   8%|████████▊                                                                                                            | 3/40 [00:45<07:03, 11.46s/it]",
      "\rtrain_batch (0.689):  10%|███████████▋                                                                                                         | 4/40 [00:45<06:51, 11.43s/it]",
      "\rtrain_batch (0.626):  10%|███████████▋                                                                                                         | 4/40 [00:57<06:51, 11.43s/it]",
      "\rtrain_batch (0.626):  12%|██████████████▋                                                                                                      | 5/40 [00:57<06:39, 11.42s/it]",
      "\rtrain_batch (0.644):  12%|██████████████▋                                                                                                      | 5/40 [01:08<06:39, 11.42s/it]",
      "\rtrain_batch (0.644):  15%|█████████████████▌                                                                                                   | 6/40 [01:08<06:28, 11.41s/it]",
      "\rtrain_batch (0.684):  15%|█████████████████▌                                                                                                   | 6/40 [01:19<06:28, 11.41s/it]",
      "\rtrain_batch (0.684):  18%|████████████████████▍                                                                                                | 7/40 [01:19<06:16, 11.40s/it]",
      "\rtrain_batch (0.670):  18%|████████████████████▍                                                                                                | 7/40 [01:31<06:16, 11.40s/it]",
      "\rtrain_batch (0.670):  20%|███████████████████████▍                                                                                             | 8/40 [01:31<06:04, 11.40s/it]",
      "\rtrain_batch (0.658):  20%|███████████████████████▍                                                                                             | 8/40 [01:42<06:04, 11.40s/it]",
      "\rtrain_batch (0.658):  22%|██████████████████████████▎                                                                                          | 9/40 [01:42<05:53, 11.39s/it]",
      "\rtrain_batch (0.648):  22%|██████████████████████████▎                                                                                          | 9/40 [01:54<05:53, 11.39s/it]",
      "\rtrain_batch (0.648):  25%|█████████████████████████████                                                                                       | 10/40 [01:54<05:42, 11.42s/it]",
      "\rtrain_batch (0.641):  25%|█████████████████████████████                                                                                       | 10/40 [02:05<05:42, 11.42s/it]",
      "\rtrain_batch (0.641):  28%|███████████████████████████████▉                                                                                    | 11/40 [02:05<05:30, 11.41s/it]",
      "\rtrain_batch (0.656):  28%|███████████████████████████████▉                                                                                    | 11/40 [02:16<05:30, 11.41s/it]",
      "\rtrain_batch (0.656):  30%|██████████████████████████████████▊                                                                                 | 12/40 [02:16<05:19, 11.41s/it]",
      "\rtrain_batch (0.678):  30%|██████████████████████████████████▊                                                                                 | 12/40 [02:28<05:19, 11.41s/it]",
      "\rtrain_batch (0.678):  32%|█████████████████████████████████████▋                                                                              | 13/40 [02:28<05:07, 11.41s/it]",
      "\rtrain_batch (0.691):  32%|█████████████████████████████████████▋                                                                              | 13/40 [02:39<05:07, 11.41s/it]",
      "\rtrain_batch (0.691):  35%|████████████████████████████████████████▌                                                                           | 14/40 [02:39<04:56, 11.39s/it]",
      "\rtrain_batch (0.654):  35%|████████████████████████████████████████▌                                                                           | 14/40 [02:51<04:56, 11.39s/it]",
      "\rtrain_batch (0.654):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [02:51<04:44, 11.39s/it]",
      "\rtrain_batch (0.667):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [03:02<04:44, 11.39s/it]",
      "\rtrain_batch (0.667):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [03:02<04:33, 11.38s/it]",
      "\rtrain_batch (0.626):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [03:13<04:33, 11.38s/it]",
      "\rtrain_batch (0.626):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [03:13<04:21, 11.38s/it]",
      "\rtrain_batch (0.670):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [03:25<04:21, 11.38s/it]",
      "\rtrain_batch (0.670):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [03:25<04:10, 11.38s/it]",
      "\rtrain_batch (0.642):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [03:36<04:10, 11.38s/it]",
      "\rtrain_batch (0.642):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [03:36<03:58, 11.38s/it]",
      "\rtrain_batch (0.603):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [03:47<03:58, 11.38s/it]",
      "\rtrain_batch (0.603):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [03:47<03:47, 11.38s/it]",
      "\rtrain_batch (0.654):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [03:59<03:47, 11.38s/it]",
      "\rtrain_batch (0.654):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [03:59<03:36, 11.37s/it]",
      "\rtrain_batch (0.627):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [04:10<03:36, 11.37s/it]",
      "\rtrain_batch (0.627):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [04:10<03:24, 11.37s/it]",
      "\rtrain_batch (0.638):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [04:22<03:24, 11.37s/it]",
      "\rtrain_batch (0.638):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [04:22<03:13, 11.36s/it]",
      "\rtrain_batch (0.639):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [04:33<03:13, 11.36s/it]",
      "\rtrain_batch (0.639):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [04:33<03:01, 11.37s/it]",
      "\rtrain_batch (0.627):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [04:44<03:01, 11.37s/it]",
      "\rtrain_batch (0.627):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [04:44<02:50, 11.38s/it]",
      "\rtrain_batch (0.683):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [04:56<02:50, 11.38s/it]",
      "\rtrain_batch (0.683):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [04:56<02:39, 11.39s/it]",
      "\rtrain_batch (0.611):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [05:07<02:39, 11.39s/it]",
      "\rtrain_batch (0.611):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [05:07<02:28, 11.40s/it]",
      "\rtrain_batch (0.608):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [05:19<02:28, 11.40s/it]",
      "\rtrain_batch (0.608):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [05:19<02:16, 11.41s/it]",
      "\rtrain_batch (0.599):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [05:30<02:16, 11.41s/it]",
      "\rtrain_batch (0.599):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [05:30<02:05, 11.40s/it]",
      "\rtrain_batch (0.649):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [05:41<02:05, 11.40s/it]",
      "\rtrain_batch (0.649):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [05:41<01:53, 11.40s/it]",
      "\rtrain_batch (0.645):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [05:53<01:53, 11.40s/it]",
      "\rtrain_batch (0.645):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [05:53<01:42, 11.40s/it]",
      "\rtrain_batch (0.667):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [06:04<01:42, 11.40s/it]",
      "\rtrain_batch (0.667):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [06:04<01:31, 11.40s/it]",
      "\rtrain_batch (0.636):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [06:16<01:31, 11.40s/it]",
      "\rtrain_batch (0.636):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [06:16<01:19, 11.40s/it]",
      "\rtrain_batch (0.626):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [06:27<01:19, 11.40s/it]",
      "\rtrain_batch (0.626):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [06:27<01:08, 11.39s/it]",
      "\rtrain_batch (0.598):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [06:38<01:08, 11.39s/it]",
      "\rtrain_batch (0.598):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [06:38<00:57, 11.40s/it]",
      "\rtrain_batch (0.688):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [06:50<00:57, 11.40s/it]",
      "\rtrain_batch (0.688):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [06:50<00:45, 11.40s/it]",
      "\rtrain_batch (0.623):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [07:01<00:45, 11.40s/it]",
      "\rtrain_batch (0.623):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [07:01<00:34, 11.42s/it]",
      "\rtrain_batch (0.651):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [07:13<00:34, 11.42s/it]",
      "\rtrain_batch (0.651):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [07:13<00:22, 11.43s/it]",
      "\rtrain_batch (0.694):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [07:24<00:22, 11.43s/it]",
      "\rtrain_batch (0.694):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [07:24<00:11, 11.42s/it]",
      "\rtrain_batch (0.618):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [07:35<00:11, 11.42s/it]",
      "\rtrain_batch (0.618): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:35<00:00, 11.41s/it]",
      "\rtrain_batch (Avg. Loss 0.648, Accuracy 72.7): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:35<00:00, 11.41s/it]",
      "\rtrain_batch (Avg. Loss 0.648, Accuracy 72.7): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [07:35<00:00, 11.40s/it]",
      "\n",
      "\rtest_batch:   0%|                                                                                                                                      | 0/40 [00:00<?, ?it/s]",
      "\rtest_batch (0.620):   0%|                                                                                                                              | 0/40 [00:09<?, ?it/s]",
      "\rtest_batch (0.620):   2%|██▉                                                                                                                   | 1/40 [00:09<06:13,  9.57s/it]",
      "\rtest_batch (0.565):   2%|██▉                                                                                                                   | 1/40 [00:19<06:13,  9.57s/it]",
      "\rtest_batch (0.565):   5%|█████▉                                                                                                                | 2/40 [00:19<06:04,  9.60s/it]",
      "\rtest_batch (0.618):   5%|█████▉                                                                                                                | 2/40 [00:28<06:04,  9.60s/it]",
      "\rtest_batch (0.618):   8%|████████▊                                                                                                             | 3/40 [00:28<05:54,  9.59s/it]",
      "\rtest_batch (0.608):   8%|████████▊                                                                                                             | 3/40 [00:38<05:54,  9.59s/it]",
      "\rtest_batch (0.608):  10%|███████████▊                                                                                                          | 4/40 [00:38<05:45,  9.59s/it]",
      "\rtest_batch (0.598):  10%|███████████▊                                                                                                          | 4/40 [00:47<05:45,  9.59s/it]",
      "\rtest_batch (0.598):  12%|██████████████▊                                                                                                       | 5/40 [00:47<05:35,  9.59s/it]",
      "\rtest_batch (0.552):  12%|██████████████▊                                                                                                       | 5/40 [00:57<05:35,  9.59s/it]",
      "\rtest_batch (0.552):  15%|█████████████████▋                                                                                                    | 6/40 [00:57<05:27,  9.62s/it]",
      "\rtest_batch (0.578):  15%|█████████████████▋                                                                                                    | 6/40 [01:07<05:27,  9.62s/it]",
      "\rtest_batch (0.578):  18%|████████████████████▋                                                                                                 | 7/40 [01:07<05:17,  9.61s/it]",
      "\rtest_batch (0.622):  18%|████████████████████▋                                                                                                 | 7/40 [01:16<05:17,  9.61s/it]",
      "\rtest_batch (0.622):  20%|███████████████████████▌                                                                                              | 8/40 [01:16<05:07,  9.60s/it]",
      "\rtest_batch (0.650):  20%|███████████████████████▌                                                                                              | 8/40 [01:26<05:07,  9.60s/it]",
      "\rtest_batch (0.650):  22%|██████████████████████████▌                                                                                           | 9/40 [01:26<04:57,  9.60s/it]",
      "\rtest_batch (0.584):  22%|██████████████████████████▌                                                                                           | 9/40 [01:36<04:57,  9.60s/it]",
      "\rtest_batch (0.584):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:36<04:48,  9.61s/it]",
      "\rtest_batch (0.546):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:45<04:48,  9.61s/it]",
      "\rtest_batch (0.546):  28%|████████████████████████████████▏                                                                                    | 11/40 [01:45<04:38,  9.61s/it]",
      "\rtest_batch (0.533):  28%|████████████████████████████████▏                                                                                    | 11/40 [01:55<04:38,  9.61s/it]",
      "\rtest_batch (0.533):  30%|███████████████████████████████████                                                                                  | 12/40 [01:55<04:29,  9.61s/it]",
      "\rtest_batch (0.581):  30%|███████████████████████████████████                                                                                  | 12/40 [02:04<04:29,  9.61s/it]",
      "\rtest_batch (0.581):  32%|██████████████████████████████████████                                                                               | 13/40 [02:04<04:19,  9.61s/it]",
      "\rtest_batch (0.682):  32%|██████████████████████████████████████                                                                               | 13/40 [02:14<04:19,  9.61s/it]",
      "\rtest_batch (0.682):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:14<04:09,  9.60s/it]",
      "\rtest_batch (0.630):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:24<04:09,  9.60s/it]",
      "\rtest_batch (0.630):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:24<03:59,  9.59s/it]",
      "\rtest_batch (0.604):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:33<03:59,  9.59s/it]",
      "\rtest_batch (0.604):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:33<03:49,  9.58s/it]",
      "\rtest_batch (0.589):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:43<03:49,  9.58s/it]",
      "\rtest_batch (0.589):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [02:43<03:40,  9.59s/it]",
      "\rtest_batch (0.552):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [02:52<03:40,  9.59s/it]",
      "\rtest_batch (0.552):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [02:52<03:30,  9.58s/it]",
      "\rtest_batch (0.537):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [03:02<03:30,  9.58s/it]",
      "\rtest_batch (0.537):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:02<03:21,  9.58s/it]",
      "\rtest_batch (0.576):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:11<03:21,  9.58s/it]",
      "\rtest_batch (0.576):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:11<03:11,  9.59s/it]",
      "\rtest_batch (0.617):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:21<03:11,  9.59s/it]",
      "\rtest_batch (0.617):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:21<03:02,  9.59s/it]",
      "\rtest_batch (0.608):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:31<03:02,  9.59s/it]",
      "\rtest_batch (0.608):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:31<02:52,  9.58s/it]",
      "\rtest_batch (0.659):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:40<02:52,  9.58s/it]",
      "\rtest_batch (0.659):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [03:40<02:43,  9.59s/it]",
      "\rtest_batch (0.648):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [03:50<02:43,  9.59s/it]",
      "\rtest_batch (0.648):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [03:50<02:33,  9.59s/it]",
      "\rtest_batch (0.620):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [03:59<02:33,  9.59s/it]",
      "\rtest_batch (0.620):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [03:59<02:23,  9.59s/it]",
      "\rtest_batch (0.569):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [04:09<02:23,  9.59s/it]",
      "\rtest_batch (0.569):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:09<02:14,  9.59s/it]",
      "\rtest_batch (0.569):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:19<02:14,  9.59s/it]",
      "\rtest_batch (0.569):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:19<02:04,  9.58s/it]",
      "\rtest_batch (0.539):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:28<02:04,  9.58s/it]",
      "\rtest_batch (0.539):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:28<01:55,  9.59s/it]",
      "\rtest_batch (0.598):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:38<01:55,  9.59s/it]",
      "\rtest_batch (0.598):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [04:38<01:45,  9.59s/it]",
      "\rtest_batch (0.624):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [04:47<01:45,  9.59s/it]",
      "\rtest_batch (0.624):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [04:47<01:35,  9.59s/it]",
      "\rtest_batch (0.663):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [04:57<01:35,  9.59s/it]",
      "\rtest_batch (0.663):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [04:57<01:26,  9.59s/it]",
      "\rtest_batch (0.594):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [05:06<01:26,  9.59s/it]",
      "\rtest_batch (0.594):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:06<01:16,  9.59s/it]",
      "\rtest_batch (0.644):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:16<01:16,  9.59s/it]",
      "\rtest_batch (0.644):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:16<01:07,  9.58s/it]",
      "\rtest_batch (0.775):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:26<01:07,  9.58s/it]",
      "\rtest_batch (0.775):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [05:26<00:57,  9.58s/it]",
      "\rtest_batch (0.676):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [05:35<00:57,  9.58s/it]",
      "\rtest_batch (0.676):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [05:35<00:47,  9.58s/it]",
      "\rtest_batch (0.602):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [05:45<00:47,  9.58s/it]",
      "\rtest_batch (0.602):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [05:45<00:38,  9.58s/it]",
      "\rtest_batch (0.553):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [05:54<00:38,  9.58s/it]",
      "\rtest_batch (0.553):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [05:54<00:28,  9.59s/it]",
      "\rtest_batch (0.728):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [06:04<00:28,  9.59s/it]",
      "\rtest_batch (0.728):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:04<00:19,  9.59s/it]",
      "\rtest_batch (0.700):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:14<00:19,  9.59s/it]",
      "\rtest_batch (0.700):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:14<00:09,  9.60s/it]",
      "\rtest_batch (0.602):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:23<00:09,  9.60s/it]",
      "\rtest_batch (0.602): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:23<00:00,  9.59s/it]",
      "\rtest_batch (Avg. Loss 0.610, Accuracy 80.0): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:23<00:00,  9.59s/it]",
      "\rtest_batch (Avg. Loss 0.610, Accuracy 80.0): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:23<00:00,  9.59s/it]",
      "\n",
      "*** Saved checkpoint finetuned_last_2.pt at epoch 2\n",
      "best acc: 80.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from hw3 import training\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# fit your model\n",
    "if not os.path.exists('finetuned_last_2.pt'):\n",
    "    trainer = training.FineTuningTrainer(model, loss_fn = None, optimizer = optimizer)\n",
    "    fit_result = trainer.fit(dl_train,dl_test, checkpoints='finetuned_last_2', num_epochs=2, max_batches= 40)\n",
    "    with open('fit_result_finetune_2.pkl', 'wb') as f:\n",
    "        pickle.dump(fit_result, f)\n",
    "    \n",
    "\n",
    "saved_state = torch.load('finetuned_last_2.pt')\n",
    "model.load_state_dict(saved_state['model_state']) \n",
    "best_acc = saved_state['best_acc']\n",
    "print('best acc:', best_acc)\n",
    "\n",
    "with open('fit_result_finetune_2.pkl', 'rb') as f:\n",
    "    fit_result = pickle.load(f) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c6147f6-87e8-4615-af5d-310e3f3cf824",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep_learning/hw3/cs236781/plot.py:118\u001b[0m, in \u001b[0;36mplot_fit\u001b[0;34m(fit_res, fig, log_loss, legend)\u001b[0m\n\u001b[1;32m    116\u001b[0m attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraintest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlossacc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fit_res, attr)\n\u001b[0;32m--> 118\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(attr)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lossacc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    493\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 494\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m index_of(xy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1348\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw3/lib/python3.8/site-packages/torch/_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ],
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": "<Figure size 1600x1000 with 4 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_fit(fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a1dc9-4f61-43b2-bfd7-0b90a6c8edf1",
   "metadata": {},
   "source": [
    "### Fine-tuning method 2 \n",
    "__Retraining all the parameters in the model__\n",
    "\n",
    "We will reload the model to ensure that the parameters are untouched and we are starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5195a86c-0ed8-4b23-8c1d-3d464c0e8783",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bf7042f-5a6f-4a92-9cd7-7ec368e76508",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37b27c9c-bba4-4533-94c7-1ce9bb6ddcc6",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 44
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95115bc4-7c7a-456c-bcbb-1017556b12a4",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Make sure all the model parameters are unfrozen\n",
    "# ====== YOUR CODE: ======\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4bcdb-07ec-4ad1-8eeb-ed9278498cf1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "--- EPOCH 1/2 ---\n",
      "\rtrain_batch:   0%|                                                                                                                                     | 0/40 [00:00<?, ?it/s]",
      "\rtrain_batch (0.683):   0%|                                                                                                                             | 0/40 [00:33<?, ?it/s]",
      "\rtrain_batch (0.683):   2%|██▉                                                                                                                  | 1/40 [00:33<21:52, 33.66s/it]",
      "\rtrain_batch (0.674):   2%|██▉                                                                                                                  | 1/40 [01:07<21:52, 33.66s/it]",
      "\rtrain_batch (0.674):   5%|█████▊                                                                                                               | 2/40 [01:07<21:20, 33.71s/it]",
      "\rtrain_batch (0.737):   5%|█████▊                                                                                                               | 2/40 [01:40<21:20, 33.71s/it]",
      "\rtrain_batch (0.737):   8%|████████▊                                                                                                            | 3/40 [01:40<20:44, 33.64s/it]",
      "\rtrain_batch (0.656):   8%|████████▊                                                                                                            | 3/40 [02:14<20:44, 33.64s/it]",
      "\rtrain_batch (0.656):  10%|███████████▋                                                                                                         | 4/40 [02:14<20:03, 33.43s/it]",
      "\rtrain_batch (0.715):  10%|███████████▋                                                                                                         | 4/40 [02:47<20:03, 33.43s/it]",
      "\rtrain_batch (0.715):  12%|██████████████▋                                                                                                      | 5/40 [02:47<19:34, 33.55s/it]",
      "\rtrain_batch (0.629):  12%|██████████████▋                                                                                                      | 5/40 [03:21<19:34, 33.55s/it]",
      "\rtrain_batch (0.629):  15%|█████████████████▌                                                                                                   | 6/40 [03:21<18:58, 33.48s/it]",
      "\rtrain_batch (0.610):  15%|█████████████████▌                                                                                                   | 6/40 [03:54<18:58, 33.48s/it]",
      "\rtrain_batch (0.610):  18%|████████████████████▍                                                                                                | 7/40 [03:54<18:20, 33.36s/it]",
      "\rtrain_batch (0.602):  18%|████████████████████▍                                                                                                | 7/40 [04:28<18:20, 33.36s/it]",
      "\rtrain_batch (0.602):  20%|███████████████████████▍                                                                                             | 8/40 [04:28<18:00, 33.75s/it]",
      "\rtrain_batch (0.677):  20%|███████████████████████▍                                                                                             | 8/40 [05:01<18:00, 33.75s/it]",
      "\rtrain_batch (0.677):  22%|██████████████████████████▎                                                                                          | 9/40 [05:01<17:20, 33.55s/it]",
      "\rtrain_batch (0.722):  22%|██████████████████████████▎                                                                                          | 9/40 [05:35<17:20, 33.55s/it]",
      "\rtrain_batch (0.722):  25%|█████████████████████████████                                                                                       | 10/40 [05:35<16:42, 33.41s/it]",
      "\rtrain_batch (0.681):  25%|█████████████████████████████                                                                                       | 10/40 [06:07<16:42, 33.41s/it]",
      "\rtrain_batch (0.681):  28%|███████████████████████████████▉                                                                                    | 11/40 [06:07<16:04, 33.25s/it]",
      "\rtrain_batch (0.682):  28%|███████████████████████████████▉                                                                                    | 11/40 [06:41<16:04, 33.25s/it]",
      "\rtrain_batch (0.682):  30%|██████████████████████████████████▊                                                                                 | 12/40 [06:41<15:34, 33.36s/it]",
      "\rtrain_batch (0.734):  30%|██████████████████████████████████▊                                                                                 | 12/40 [07:14<15:34, 33.36s/it]",
      "\rtrain_batch (0.734):  32%|█████████████████████████████████████▋                                                                              | 13/40 [07:14<14:56, 33.19s/it]",
      "\rtrain_batch (0.616):  32%|█████████████████████████████████████▋                                                                              | 13/40 [07:49<14:56, 33.19s/it]",
      "\rtrain_batch (0.616):  35%|████████████████████████████████████████▌                                                                           | 14/40 [07:49<14:35, 33.66s/it]",
      "\rtrain_batch (0.659):  35%|████████████████████████████████████████▌                                                                           | 14/40 [08:22<14:35, 33.66s/it]",
      "\rtrain_batch (0.659):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [08:22<14:02, 33.70s/it]",
      "\rtrain_batch (0.667):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [08:56<14:02, 33.70s/it]",
      "\rtrain_batch (0.667):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [08:56<13:26, 33.60s/it]",
      "\rtrain_batch (0.640):  40%|██████████████████████████████████████████████▍                                                                     | 16/40 [09:30<13:26, 33.60s/it]",
      "\rtrain_batch (0.640):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [09:30<12:55, 33.73s/it]",
      "\rtrain_batch (0.639):  42%|█████████████████████████████████████████████████▎                                                                  | 17/40 [10:04<12:55, 33.73s/it]",
      "\rtrain_batch (0.639):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [10:04<12:25, 33.91s/it]",
      "\rtrain_batch (0.658):  45%|████████████████████████████████████████████████████▏                                                               | 18/40 [10:37<12:25, 33.91s/it]",
      "\rtrain_batch (0.658):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [10:37<11:46, 33.65s/it]",
      "\rtrain_batch (0.616):  48%|███████████████████████████████████████████████████████                                                             | 19/40 [11:10<11:46, 33.65s/it]",
      "\rtrain_batch (0.616):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [11:10<11:06, 33.31s/it]",
      "\rtrain_batch (0.635):  50%|██████████████████████████████████████████████████████████                                                          | 20/40 [11:42<11:06, 33.31s/it]",
      "\rtrain_batch (0.635):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [11:42<10:28, 33.09s/it]",
      "\rtrain_batch (0.674):  52%|████████████████████████████████████████████████████████████▉                                                       | 21/40 [12:15<10:28, 33.09s/it]",
      "\rtrain_batch (0.674):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [12:15<09:51, 32.87s/it]",
      "\rtrain_batch (0.585):  55%|███████████████████████████████████████████████████████████████▊                                                    | 22/40 [12:47<09:51, 32.87s/it]",
      "\rtrain_batch (0.585):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [12:47<09:15, 32.69s/it]",
      "\rtrain_batch (0.562):  57%|██████████████████████████████████████████████████████████████████▋                                                 | 23/40 [13:19<09:15, 32.69s/it]",
      "\rtrain_batch (0.562):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [13:19<08:40, 32.51s/it]",
      "\rtrain_batch (0.537):  60%|█████████████████████████████████████████████████████████████████████▌                                              | 24/40 [13:51<08:40, 32.51s/it]",
      "\rtrain_batch (0.537):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [13:51<08:06, 32.46s/it]",
      "\rtrain_batch (0.481):  62%|████████████████████████████████████████████████████████████████████████▌                                           | 25/40 [14:24<08:06, 32.46s/it]",
      "\rtrain_batch (0.481):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [14:24<07:34, 32.49s/it]",
      "\rtrain_batch (0.490):  65%|███████████████████████████████████████████████████████████████████████████▍                                        | 26/40 [14:59<07:34, 32.49s/it]",
      "\rtrain_batch (0.490):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [14:59<07:14, 33.42s/it]",
      "\rtrain_batch (0.505):  68%|██████████████████████████████████████████████████████████████████████████████▎                                     | 27/40 [15:39<07:14, 33.42s/it]",
      "\rtrain_batch (0.505):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [15:39<07:04, 35.34s/it]",
      "\rtrain_batch (0.474):  70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 28/40 [16:14<07:04, 35.34s/it]",
      "\rtrain_batch (0.474):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [16:14<06:25, 35.08s/it]",
      "\rtrain_batch (0.618):  72%|████████████████████████████████████████████████████████████████████████████████████                                | 29/40 [16:47<06:25, 35.08s/it]",
      "\rtrain_batch (0.618):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [16:47<05:44, 34.43s/it]",
      "\rtrain_batch (0.319):  75%|███████████████████████████████████████████████████████████████████████████████████████                             | 30/40 [17:20<05:44, 34.43s/it]",
      "\rtrain_batch (0.319):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [17:20<05:05, 33.96s/it]",
      "\rtrain_batch (0.484):  78%|█████████████████████████████████████████████████████████████████████████████████████████▉                          | 31/40 [17:52<05:05, 33.96s/it]",
      "\rtrain_batch (0.484):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [17:52<04:28, 33.62s/it]",
      "\rtrain_batch (0.682):  80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 32/40 [18:25<04:28, 33.62s/it]",
      "\rtrain_batch (0.682):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [18:25<03:53, 33.41s/it]",
      "\rtrain_batch (0.918):  82%|███████████████████████████████████████████████████████████████████████████████████████████████▋                    | 33/40 [18:58<03:53, 33.41s/it]",
      "\rtrain_batch (0.918):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [18:58<03:19, 33.29s/it]",
      "\rtrain_batch (0.603):  85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/40 [19:31<03:19, 33.29s/it]",
      "\rtrain_batch (0.603):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [19:31<02:45, 33.19s/it]",
      "\rtrain_batch (0.369):  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 35/40 [20:05<02:45, 33.19s/it]",
      "\rtrain_batch (0.369):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [20:05<02:13, 33.31s/it]",
      "\rtrain_batch (0.254):  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 36/40 [20:38<02:13, 33.31s/it]",
      "\rtrain_batch (0.254):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [20:38<01:39, 33.23s/it]",
      "\rtrain_batch (0.454):  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 37/40 [21:13<01:39, 33.23s/it]",
      "\rtrain_batch (0.454):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [21:13<01:07, 33.68s/it]",
      "\rtrain_batch (0.277):  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [21:46<01:07, 33.68s/it]",
      "\rtrain_batch (0.277):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [21:46<00:33, 33.63s/it]",
      "\rtrain_batch (0.331):  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [22:20<00:33, 33.63s/it]",
      "\rtrain_batch (0.331): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [22:20<00:00, 33.70s/it]",
      "\rtrain_batch (Avg. Loss 0.589, Accuracy 68.8): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [22:20<00:00, 33.70s/it]",
      "\rtrain_batch (Avg. Loss 0.589, Accuracy 68.8): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [22:20<00:00, 33.51s/it]",
      "\n",
      "\rtest_batch:   0%|                                                                                                                                      | 0/40 [00:00<?, ?it/s]",
      "\rtest_batch (0.147):   0%|                                                                                                                              | 0/40 [00:10<?, ?it/s]",
      "\rtest_batch (0.147):   2%|██▉                                                                                                                   | 1/40 [00:10<06:43, 10.34s/it]",
      "\rtest_batch (0.209):   2%|██▉                                                                                                                   | 1/40 [00:20<06:43, 10.34s/it]",
      "\rtest_batch (0.209):   5%|█████▉                                                                                                                | 2/40 [00:20<06:28, 10.21s/it]",
      "\rtest_batch (0.408):   5%|█████▉                                                                                                                | 2/40 [00:30<06:28, 10.21s/it]",
      "\rtest_batch (0.408):   8%|████████▊                                                                                                             | 3/40 [00:30<06:18, 10.23s/it]",
      "\rtest_batch (0.475):   8%|████████▊                                                                                                             | 3/40 [00:41<06:18, 10.23s/it]",
      "\rtest_batch (0.475):  10%|███████████▊                                                                                                          | 4/40 [00:41<06:09, 10.26s/it]",
      "\rtest_batch (0.345):  10%|███████████▊                                                                                                          | 4/40 [00:51<06:09, 10.26s/it]",
      "\rtest_batch (0.345):  12%|██████████████▊                                                                                                       | 5/40 [00:51<05:59, 10.28s/it]",
      "\rtest_batch (0.359):  12%|██████████████▊                                                                                                       | 5/40 [01:01<05:59, 10.28s/it]",
      "\rtest_batch (0.359):  15%|█████████████████▋                                                                                                    | 6/40 [01:01<05:53, 10.40s/it]",
      "\rtest_batch (0.300):  15%|█████████████████▋                                                                                                    | 6/40 [01:12<05:53, 10.40s/it]",
      "\rtest_batch (0.300):  18%|████████████████████▋                                                                                                 | 7/40 [01:12<05:42, 10.37s/it]",
      "\rtest_batch (0.471):  18%|████████████████████▋                                                                                                 | 7/40 [01:22<05:42, 10.37s/it]",
      "\rtest_batch (0.471):  20%|███████████████████████▌                                                                                              | 8/40 [01:22<05:31, 10.36s/it]",
      "\rtest_batch (0.592):  20%|███████████████████████▌                                                                                              | 8/40 [01:32<05:31, 10.36s/it]",
      "\rtest_batch (0.592):  22%|██████████████████████████▌                                                                                           | 9/40 [01:32<05:20, 10.34s/it]",
      "\rtest_batch (0.285):  22%|██████████████████████████▌                                                                                           | 9/40 [01:43<05:20, 10.34s/it]",
      "\rtest_batch (0.285):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:43<05:10, 10.34s/it]",
      "\rtest_batch (0.136):  25%|█████████████████████████████▎                                                                                       | 10/40 [01:53<05:10, 10.34s/it]",
      "\rtest_batch (0.136):  28%|████████████████████████████████▏                                                                                    | 11/40 [01:53<04:59, 10.32s/it]",
      "\rtest_batch (0.129):  28%|████████████████████████████████▏                                                                                    | 11/40 [02:03<04:59, 10.32s/it]",
      "\rtest_batch (0.129):  30%|███████████████████████████████████                                                                                  | 12/40 [02:03<04:49, 10.35s/it]",
      "\rtest_batch (0.231):  30%|███████████████████████████████████                                                                                  | 12/40 [02:14<04:49, 10.35s/it]",
      "\rtest_batch (0.231):  32%|██████████████████████████████████████                                                                               | 13/40 [02:14<04:38, 10.32s/it]",
      "\rtest_batch (0.388):  32%|██████████████████████████████████████                                                                               | 13/40 [02:24<04:38, 10.32s/it]",
      "\rtest_batch (0.388):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:24<04:28, 10.31s/it]",
      "\rtest_batch (0.255):  35%|████████████████████████████████████████▉                                                                            | 14/40 [02:34<04:28, 10.31s/it]",
      "\rtest_batch (0.255):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:34<04:17, 10.29s/it]",
      "\rtest_batch (0.375):  38%|███████████████████████████████████████████▉                                                                         | 15/40 [02:45<04:17, 10.29s/it]",
      "\rtest_batch (0.375):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:45<04:08, 10.34s/it]",
      "\rtest_batch (0.400):  40%|██████████████████████████████████████████████▊                                                                      | 16/40 [02:55<04:08, 10.34s/it]",
      "\rtest_batch (0.400):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [02:55<03:58, 10.38s/it]",
      "\rtest_batch (0.183):  42%|█████████████████████████████████████████████████▋                                                                   | 17/40 [03:05<03:58, 10.38s/it]",
      "\rtest_batch (0.183):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [03:05<03:47, 10.35s/it]",
      "\rtest_batch (0.172):  45%|████████████████████████████████████████████████████▋                                                                | 18/40 [03:16<03:47, 10.35s/it]",
      "\rtest_batch (0.172):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:16<03:37, 10.35s/it]",
      "\rtest_batch (0.355):  48%|███████████████████████████████████████████████████████▌                                                             | 19/40 [03:26<03:37, 10.35s/it]",
      "\rtest_batch (0.355):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:26<03:26, 10.34s/it]",
      "\rtest_batch (0.444):  50%|██████████████████████████████████████████████████████████▌                                                          | 20/40 [03:36<03:26, 10.34s/it]",
      "\rtest_batch (0.444):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:36<03:16, 10.32s/it]",
      "\rtest_batch (0.613):  52%|█████████████████████████████████████████████████████████████▍                                                       | 21/40 [03:47<03:16, 10.32s/it]",
      "\rtest_batch (0.613):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:47<03:05, 10.30s/it]",
      "\rtest_batch (0.421):  55%|████████████████████████████████████████████████████████████████▎                                                    | 22/40 [03:57<03:05, 10.30s/it]",
      "\rtest_batch (0.421):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [03:57<02:55, 10.33s/it]",
      "\rtest_batch (0.474):  57%|███████████████████████████████████████████████████████████████████▎                                                 | 23/40 [04:07<02:55, 10.33s/it]",
      "\rtest_batch (0.474):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [04:07<02:44, 10.31s/it]",
      "\rtest_batch (0.188):  60%|██████████████████████████████████████████████████████████████████████▏                                              | 24/40 [04:18<02:44, 10.31s/it]",
      "\rtest_batch (0.188):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [04:18<02:34, 10.31s/it]",
      "\rtest_batch (0.300):  62%|█████████████████████████████████████████████████████████████████████████▏                                           | 25/40 [04:28<02:34, 10.31s/it]",
      "\rtest_batch (0.300):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:28<02:24, 10.33s/it]",
      "\rtest_batch (0.108):  65%|████████████████████████████████████████████████████████████████████████████                                         | 26/40 [04:38<02:24, 10.33s/it]",
      "\rtest_batch (0.108):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:38<02:14, 10.32s/it]",
      "\rtest_batch (0.118):  68%|██████████████████████████████████████████████████████████████████████████████▉                                      | 27/40 [04:49<02:14, 10.32s/it]",
      "\rtest_batch (0.118):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:49<02:03, 10.32s/it]",
      "\rtest_batch (0.213):  70%|█████████████████████████████████████████████████████████████████████████████████▉                                   | 28/40 [04:59<02:03, 10.32s/it]",
      "\rtest_batch (0.213):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [04:59<01:53, 10.32s/it]",
      "\rtest_batch (0.406):  72%|████████████████████████████████████████████████████████████████████████████████████▊                                | 29/40 [05:09<01:53, 10.32s/it]",
      "\rtest_batch (0.406):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [05:09<01:43, 10.35s/it]",
      "\rtest_batch (0.261):  75%|███████████████████████████████████████████████████████████████████████████████████████▊                             | 30/40 [05:20<01:43, 10.35s/it]",
      "\rtest_batch (0.261):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [05:20<01:33, 10.39s/it]",
      "\rtest_batch (0.204):  78%|██████████████████████████████████████████████████████████████████████████████████████████▋                          | 31/40 [05:30<01:33, 10.39s/it]",
      "\rtest_batch (0.204):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:30<01:23, 10.39s/it]",
      "\rtest_batch (0.331):  80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 32/40 [05:41<01:23, 10.39s/it]",
      "\rtest_batch (0.331):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:41<01:13, 10.45s/it]",
      "\rtest_batch (0.896):  82%|████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 33/40 [05:52<01:13, 10.45s/it]",
      "\rtest_batch (0.896):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [05:52<01:03, 10.55s/it]",
      "\rtest_batch (0.566):  85%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 34/40 [06:03<01:03, 10.55s/it]",
      "\rtest_batch (0.566):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [06:03<00:53, 10.71s/it]",
      "\rtest_batch (0.323):  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 35/40 [06:13<00:53, 10.71s/it]",
      "\rtest_batch (0.323):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [06:13<00:42, 10.60s/it]",
      "\rtest_batch (0.166):  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 36/40 [06:23<00:42, 10.60s/it]",
      "\rtest_batch (0.166):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [06:23<00:31, 10.50s/it]",
      "\rtest_batch (0.475):  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 37/40 [06:34<00:31, 10.50s/it]",
      "\rtest_batch (0.475):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:34<00:20, 10.45s/it]",
      "\rtest_batch (0.505):  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 38/40 [06:45<00:20, 10.45s/it]",
      "\rtest_batch (0.505):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:45<00:10, 10.66s/it]",
      "\rtest_batch (0.399):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 39/40 [06:55<00:10, 10.66s/it]",
      "\rtest_batch (0.399): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:55<00:00, 10.57s/it]",
      "\rtest_batch (Avg. Loss 0.341, Accuracy 85.4): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:55<00:00, 10.57s/it]",
      "\rtest_batch (Avg. Loss 0.341, Accuracy 85.4): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [06:55<00:00, 10.39s/it]",
      "\n",
      "*** Saved checkpoint finetuned_all.pt at epoch 1\n--- EPOCH 2/2 ---\n",
      "\rtrain_batch:   0%|                                                                                                                                     | 0/40 [00:00<?, ?it/s]",
      "\rtrain_batch (0.177):   0%|                                                                                                                             | 0/40 [00:34<?, ?it/s]",
      "\rtrain_batch (0.177):   2%|██▉                                                                                                                  | 1/40 [00:34<22:34, 34.73s/it]",
      "\rtrain_batch (0.320):   2%|██▉                                                                                                                  | 1/40 [01:08<22:34, 34.73s/it]",
      "\rtrain_batch (0.320):   5%|█████▊                                                                                                               | 2/40 [01:08<21:43, 34.31s/it]",
      "\rtrain_batch (0.403):   5%|█████▊                                                                                                               | 2/40 [01:42<21:43, 34.31s/it]",
      "\rtrain_batch (0.403):   8%|████████▊                                                                                                            | 3/40 [01:42<21:07, 34.25s/it]",
      "\rtrain_batch (0.187):   8%|████████▊                                                                                                            | 3/40 [02:18<21:07, 34.25s/it]",
      "\rtrain_batch (0.187):  10%|███████████▋                                                                                                         | 4/40 [02:18<20:46, 34.62s/it]",
      "\rtrain_batch (0.503):  10%|███████████▋                                                                                                         | 4/40 [02:53<20:46, 34.62s/it]",
      "\rtrain_batch (0.503):  12%|██████████████▋                                                                                                      | 5/40 [02:53<20:22, 34.93s/it]",
      "\rtrain_batch (0.650):  12%|██████████████▋                                                                                                      | 5/40 [03:28<20:22, 34.93s/it]",
      "\rtrain_batch (0.650):  15%|█████████████████▌                                                                                                   | 6/40 [03:28<19:47, 34.93s/it]",
      "\rtrain_batch (0.291):  15%|█████████████████▌                                                                                                   | 6/40 [04:04<19:47, 34.93s/it]",
      "\rtrain_batch (0.291):  18%|████████████████████▍                                                                                                | 7/40 [04:04<19:21, 35.19s/it]",
      "\rtrain_batch (0.255):  18%|████████████████████▍                                                                                                | 7/40 [04:39<19:21, 35.19s/it]",
      "\rtrain_batch (0.255):  20%|███████████████████████▍                                                                                             | 8/40 [04:39<18:48, 35.27s/it]",
      "\rtrain_batch (0.326):  20%|███████████████████████▍                                                                                             | 8/40 [05:15<18:48, 35.27s/it]",
      "\rtrain_batch (0.326):  22%|██████████████████████████▎                                                                                          | 9/40 [05:15<18:17, 35.41s/it]",
      "\rtrain_batch (0.363):  22%|██████████████████████████▎                                                                                          | 9/40 [05:50<18:17, 35.41s/it]",
      "\rtrain_batch (0.363):  25%|█████████████████████████████                                                                                       | 10/40 [05:50<17:39, 35.31s/it]",
      "\rtrain_batch (0.380):  25%|█████████████████████████████                                                                                       | 10/40 [06:25<17:39, 35.31s/it]",
      "\rtrain_batch (0.380):  28%|███████████████████████████████▉                                                                                    | 11/40 [06:25<17:00, 35.20s/it]",
      "\rtrain_batch (0.287):  28%|███████████████████████████████▉                                                                                    | 11/40 [06:58<17:00, 35.20s/it]",
      "\rtrain_batch (0.287):  30%|██████████████████████████████████▊                                                                                 | 12/40 [06:58<16:10, 34.68s/it]",
      "\rtrain_batch (0.136):  30%|██████████████████████████████████▊                                                                                 | 12/40 [07:32<16:10, 34.68s/it]",
      "\rtrain_batch (0.136):  32%|█████████████████████████████████████▋                                                                              | 13/40 [07:32<15:25, 34.27s/it]",
      "\rtrain_batch (0.461):  32%|█████████████████████████████████████▋                                                                              | 13/40 [08:05<15:25, 34.27s/it]",
      "\rtrain_batch (0.461):  35%|████████████████████████████████████████▌                                                                           | 14/40 [08:05<14:41, 33.89s/it]",
      "\rtrain_batch (0.459):  35%|████████████████████████████████████████▌                                                                           | 14/40 [08:42<14:41, 33.89s/it]",
      "\rtrain_batch (0.459):  38%|███████████████████████████████████████████▌                                                                        | 15/40 [08:42<14:33, 34.92s/it]"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# fit your model\n",
    "if not os.path.exists('finetuned_all.pt'):\n",
    "    trainer = training.FineTuningTrainer(model, loss_fn = None, optimizer = optimizer)\n",
    "    fit_result = trainer.fit(dl_train,dl_test, checkpoints='finetuned_all', num_epochs=2, max_batches= 40)\n",
    "    with open('finetuned_all.pkl', 'wb') as f:\n",
    "        pickle.dump(fit_result, f)\n",
    "    \n",
    "\n",
    "saved_state = torch.load('finetuned_all.pt')\n",
    "model.load_state_dict(saved_state['model_state']) \n",
    "\n",
    "with open('finetuned_all.pkl', 'rb') as f:\n",
    "    fit_result = pickle.load(f)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1891724-abac-4b07-907e-6d4eddf28f97",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_fit(fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45272fa-7485-442f-9b55-02e915f5996e",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ba155-e730-40ac-b3d4-b70acce56129",
   "metadata": {},
   "source": [
    "Fill out your answers in `hw3.answers.part4_q1` and `hw3.answers.part4_q2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bc948-eaaa-42b7-a9cf-e5f5907b48fa",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175a5b9-f10c-40f8-ab04-a4b98cff300b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151329aa-8c63-4160-ba05-26b04f16671f",
   "metadata": {},
   "source": [
    "Explain the results that you got here in comparison to the results achieved in the *trained from scratch* encoder from the preivous part.  \n",
    "If one of the models performed better, why was this so?   \n",
    "Will this always be the case on any downstream task, or was this phenomenom specific to this task?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133cc62-19d4-4928-b7b6-a30bfd891b1f",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part4_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193f04-6678-4837-b10f-517ca09436d9",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05c554-35ca-4229-8ec1-ca2a901c6f9a",
   "metadata": {},
   "source": [
    "Assume that when fine-tuning, instead of freezing the last two linear layers, you instead froze some other internal model layers, such as the multi-headed attention blocks.  \n",
    "Would the model still be able to succesfully fine-tune to this task?   \n",
    "Or would the results be worse?  \n",
    "Explain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20da29-d63b-4c1c-a941-b44c5e22f92e",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part4_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e8206-7a47-4ab2-ad37-c1495afbdd33",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}